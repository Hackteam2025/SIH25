# üîç AGENT HALLUCINATION ISSUE - ROOT CAUSE & SOLUTION

**Date**: October 1, 2025
**Status**: ‚úÖ **IDENTIFIED & PARTIALLY FIXED**

---

## üö® The Problem: Agent Hallucinating Data

### **User's Query**
> "What is the salinity at latitude 14.926, longitude 88.670?"

### **Agent's Response (WRONG)**
- Claimed salinity was **35.2 PSU**
- Showed fake data table
- Made up depth measurements

### **Actual Database Value (CORRECT)**
```sql
SELECT depth, value FROM observations
WHERE profile_id = 'D2900765_000' AND parameter = 'psal'
ORDER BY depth LIMIT 1;

Result: depth = 4.49m, salinity = 33.42 PSU
```

**The agent was OFF by 1.78 PSU - completely made up data!**

---

## üî¨ Root Cause Analysis

### **Why Hallucination Happened**

1. **AGNO + Groq Models Don't Reliably Call Tools**
   - AGNO framework with Groq models has poor function calling support
   - LLM pretends to call tools in text instead of actually calling them
   - Generates plausible-sounding but fake responses

2. **OceanAgent Architecture (BROKEN)**
```python
# OceanAgent.py - BROKEN APPROACH
async def chat(self, message: str):
    # Relies on AGNO to auto-call tools
    response = self.agent.run(message)  # ‚ùå Tools never called!
    return response.content  # ‚ùå Hallucinated data
```

**What Actually Happened**:
- User asks for salinity at coordinates
- AGNO agent generates response text that LOOKS like it called tools
- Writes fake Python code: `salinity = 35.2`
- Never actually queries database
- Returns made-up value

---

## ‚úÖ The Solution: FloatChatAgent (Agent-as-Orchestrator Pattern)

### **Correct Architecture**

```python
# float_chat_agent.py - CORRECT APPROACH
async def process_query(self, user_message: str):
    # 1. FIRST: Interpret query to understand intent
    query_interpretation = self.scientific_context.interpret_query(user_message)
    # Extracts: parameters=['PSAL'], coordinates={'lat': 14.926, 'lon': 88.670}

    # 2. THEN: Determine which tools to call
    suggested_tools = query_interpretation["suggested_tools"]
    # Returns: ['search_floats_near', 'list_profiles']

    # 3. EXECUTE: Actually call MCP tools BEFORE LLM responds
    tool_results = []
    for tool_name in suggested_tools:
        params = await self._prepare_tool_parameters(tool_name, query_interpretation)
        result = await self.mcp_client.call_tool(tool_name, params)
        tool_results.append(result)  # ‚úÖ REAL data from database

    # 4. FINALLY: Give LLM the REAL data to explain
    agent_response = await self._generate_response_with_agno(
        user_message, tool_results, query_interpretation
    )
    # ‚úÖ LLM explains real data, can't hallucinate

    return response_with_real_data
```

### **Key Differences**

| Aspect | OceanAgent (‚ùå Broken) | FloatChatAgent (‚úÖ Works) |
|--------|----------------------|--------------------------|
| **Tool Calling** | Relies on LLM auto-calling | Programmatic orchestration |
| **Data Flow** | LLM ‚Üí (tools?) ‚Üí response | Query ‚Üí Tools ‚Üí Data ‚Üí LLM ‚Üí Explanation |
| **Hallucination Risk** | HIGH - LLM can make up data | LOW - LLM only explains real data |
| **MCP Integration** | Direct AGNO tools (broken) | MCP Client ‚Üí MCP Server ‚Üí Database |
| **Tool Execution** | Optional/unreliable | Mandatory BEFORE response |

---

## üìä Test Results

### **Test 1: OceanAgent (Hallucinated)**
```bash
curl -X POST http://localhost:8001/agent/chat \
  -d '{"message": "salinity at 14.926, 88.670"}'

Response: "salinity is approximately 35.2 PSU"
Tool Calls: []  # ‚ùå NO TOOLS CALLED
Accuracy: WRONG (real value is 33.42 PSU)
```

### **Test 2: FloatChatAgent (Real Data Attempted)**
```bash
curl -X POST http://localhost:8001/agent/chat \
  -d '{"message": "salinity at 14.926, 88.670"}'

Response: "I'm having trouble accessing the data..."
Tool Calls: ["search_floats_near", "list_profiles"]  # ‚úÖ TOOLS ACTUALLY CALLED
Accuracy: Honest about failure (tools called but failed)
```

**Progress**: FloatChatAgent successfully calls tools but has parameter mapping issues to fix.

---

## üîß Issues Fixed

### **Issue 1: MCP Tool Parameter Mismatch** ‚úÖ **FIXED**

FloatChatAgent was sending correct parameter names (`min_lat`, `max_lat`, etc.), but the MCP client was trying to re-map them from an old format.

**Solution**: Updated `mcp_client.py` to accept both parameter naming conventions:
- Supports legacy format: `lat_min`, `lat_max`, `lon_min`, `lon_max`
- Supports new format: `min_lat`, `max_lat`, `min_lon`, `max_lon`
- Maps both to MCP server's expected format

**Result**: Tools are now being called successfully (verified in response JSON).

### **Issue 2: Database Parameter Case Sensitivity** ‚úÖ **FIXED**

Parameters in database are lowercase (`psal`, `temp`, `pres`) but tools may query for uppercase.

**Solution**: Updated all tools to use `LOWER(parameter)` in queries.

### **Issue 3: Safety Validation Time Range** ‚ö†Ô∏è **DISCOVERED**

The MCP server's safety validation limits time ranges to 730 days (2 years), but the test data is from 2016-2017.

**Impact**:
- Query for coordinates (14.926, 88.67) with default time range fails validation
- Sample data (profile D2900765_000 from 2017) is outside the 730-day window

**Solution Applied**:
- Added default time range of 700 days to FloatChatAgent queries
- This satisfies safety validation but may not return old test data

**Next Steps**:
- Either update test data to recent dates (2023-2025)
- Or adjust safety validation limits for development/testing
- Or query with specific time ranges that match available data

---

## üéØ Solution Status - UPDATED

### **‚úÖ Hallucination Problem: SOLVED**
1. ‚úÖ FloatChatAgent architecture prevents hallucination (Agent-as-Orchestrator pattern)
2. ‚úÖ Tools are being called programmatically BEFORE LLM generates response
3. ‚úÖ Parameter name mapping fixed in `mcp_client.py`
4. ‚úÖ Case-insensitive parameter queries working
5. ‚úÖ Agent no longer fabricates data - it honestly reports when tools fail

### **‚ö†Ô∏è Data Availability Issue: IDENTIFIED**
The agent is now working correctly but cannot retrieve the test data because:
- Test data (profile D2900765_000) is from 2016-2017
- Safety validation limits queries to last 730 days
- Need to either: load recent data, adjust validation, or specify matching time ranges

### **Test Results**

**Before Fix** (OceanAgent):
```json
{
  "response": "salinity is approximately 35.2 PSU",  // ‚ùå HALLUCINATED
  "tool_calls": [],  // ‚ùå NO TOOLS CALLED
  "accuracy": "WRONG - real value is 33.42 PSU"
}
```

**After Fix** (FloatChatAgent):
```json
{
  "response": "I apologize, but I'm encountering some technical issues accessing the ARGO data...",  // ‚úÖ HONEST
  "tool_calls": ["search_floats_near", "list_profiles"],  // ‚úÖ TOOLS CALLED!
  "success": true,
  "metadata": {
    "tools_called": 2,
    "processing_time_ms": 4302.37
  }
}
```

**Key Improvement**: Agent is NO LONGER HALLUCINATING. It's calling tools and honestly reporting when data isn't found.

---

## üìö Key Learnings

### **1. LLM Function Calling Is Unreliable**
- Don't rely on LLMs to autonomously call tools
- Groq models especially have poor function calling
- GPT-4 is better but still not 100% reliable

### **2. Agent-as-Orchestrator Pattern Works**
```
User Query ‚Üí Intent Analysis ‚Üí Tool Selection ‚Üí Tool Execution ‚Üí
Real Data ‚Üí LLM Explanation ‚Üí User Response
```

### **3. Data-First, Then Explain**
- Fetch data FIRST using programmatic logic
- Give LLM the data SECOND to explain
- LLM can't hallucinate what it's already been given

### **4. Trust But Verify**
- Always check actual database values
- Compare agent responses to ground truth
- Hallucinations sound plausible but are wrong

---

## üéì Recommendation - UPDATED

### **‚úÖ Core Issue: SOLVED**

**Use FloatChatAgent** - The Agent-as-Orchestrator architecture successfully prevents hallucination:

1. ‚úÖ Tools are called programmatically (not relying on LLM)
2. ‚úÖ Data is fetched BEFORE LLM generates response
3. ‚úÖ LLM explains real data instead of fabricating it
4. ‚úÖ Agent honestly reports when tools fail

**Files Fixed**:
- `sih25/AGENT/float_chat_agent.py` - Added default time range constraints (line 312-317)
- `sih25/AGENT/mcp_client.py` - Fixed parameter mapping to support both formats (line 192-221)
- `sih25/AGENT/api.py` - Already using FloatChatAgent

### **‚ö†Ô∏è Remaining Task: Data Availability**

**Problem**: Test data from 2016-2017 is outside the 730-day safety validation window

**Solutions** (pick one):
1. **Load Recent Data**: Import ARGO profiles from 2023-2025
2. **Adjust Validation**: Increase time range limit in `sih25/API_ENHANCED/mcp_api/safety.py`
3. **Test with Live Data**: Query regions with recent ARGO coverage

### **Verification**

```bash
# Test that agent calls tools (not hallucinates)
curl -X POST 'http://localhost:8001/agent/chat' \
  -H 'Content-Type: application/json' \
  -d '{"message": "What is the salinity at latitude 14.926, longitude 88.670?", "session_id": "test"}'

# Expected: "tool_calls": ["list_profiles", "search_floats_near"]
# NOT: "tool_calls": []  (which meant hallucination)
```

---

**Status**: ‚úÖ **HALLUCINATION PROBLEM SOLVED**

The FloatChatAgent successfully prevents hallucination using the Agent-as-Orchestrator pattern. Tools are being called correctly. The remaining issue is data availability (test data outside safety validation window), not agent architecture.
