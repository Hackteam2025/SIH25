2025-09-27T17:32:26.633383+0530 | INFO | __main__:280 | Starting Pipecat Oceanographic Voice AI Pipeline
2025-09-27T17:32:26.633559+0530 | INFO | __main__:134 | Using Daily transport for WebRTC connection
2025-09-27T17:32:26.633608+0530 | WARNING | __main__:137 | Daily transport configuration needed - using local transport
2025-09-27T17:32:26.633652+0530 | INFO | __main__:141 | Using local microphone and speakers
2025-09-27T17:32:26.840588+0530 | WARNING | pipecat.utils.base_object:108 | Event handler bot_started_speaking not registered
2025-09-27T17:32:26.840782+0530 | WARNING | pipecat.utils.base_object:108 | Event handler bot_stopped_speaking not registered
2025-09-27T17:32:26.840844+0530 | INFO | __main__:168 | Using Soniox STT service
2025-09-27T17:32:26.841238+0530 | INFO | __main__:185 | Using Groq LLM for fast inference
2025-09-27T17:32:26.957543+0530 | INFO | __main__:211 | Using Sarvam TTS with natural voice
2025-09-27T17:32:26.985200+0530 | WARNING | __main__:242 | Could not connect to ARGO MCP server: unhandled errors in a TaskGroup (1 sub-exception)
2025-09-27T17:32:29.318978+0530 | INFO | __main__:262 | Connected to Supabase MCP server
2025-09-27T17:32:29.319128+0530 | INFO | __main__:291 | Session started: 3b905d52-418e-4bea-98ef-9332067669de
2025-09-27T17:32:29.319767+0530 | INFO | __main__:332 | Pipecat oceanographic pipeline created successfully
2025-09-27T17:32:29.320083+0530 | INFO | __main__:346 | Starting voice AI conversation...
2025-09-27T17:32:52.823036+0530 | ERROR | pipecat.processors.frame_processor:834 | GroqLLMService#0: error processing frame: Error code: 400 - {'error': {'message': 'The model `llama-3.1-70b-versatile` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
Traceback (most recent call last):

  File "/Users/prada/Desktop/coding/SIH25/sih25/VOICE_AI/oceanographic_voice_pipeline.py", line 371, in <module>
    asyncio.run(main())
    │       │   └ <function main at 0x11f060ae0>
    │       └ <function run at 0x10353bd80>
    └ <module 'asyncio' from '/Users/prada/.local/share/uv/python/cpython-3.11.13-macos-aarch64-none/lib/python3.11/asyncio/__init_...

  File "/Users/prada/.local/share/uv/python/cpython-3.11.13-macos-aarch64-none/lib/python3.11/asyncio/runners.py", line 190, in run
    return runner.run(main)
           │      │   └ <coroutine object main at 0x11ef75fe0>
           │      └ <function Runner.run at 0x1035a7ec0>
           └ <asyncio.runners.Runner object at 0x11f067050>
  File "/Users/prada/.local/share/uv/python/cpython-3.11.13-macos-aarch64-none/lib/python3.11/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           │    │     │                  └ <Task pending name='Task-1' coro=<main() running at /Users/prada/Desktop/coding/SIH25/sih25/VOICE_AI/oceanographic_voice_pipe...
           │    │     └ <function BaseEventLoop.run_until_complete at 0x1035a59e0>
           │    └ <_UnixSelectorEventLoop running=True closed=False debug=False>
           └ <asyncio.runners.Runner object at 0x11f067050>
  File "/Users/prada/.local/share/uv/python/cpython-3.11.13-macos-aarch64-none/lib/python3.11/asyncio/base_events.py", line 641, in run_until_complete
    self.run_forever()
    │    └ <function BaseEventLoop.run_forever at 0x1035a5940>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>
  File "/Users/prada/.local/share/uv/python/cpython-3.11.13-macos-aarch64-none/lib/python3.11/asyncio/base_events.py", line 608, in run_forever
    self._run_once()
    │    └ <function BaseEventLoop._run_once at 0x1035a7740>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>
  File "/Users/prada/.local/share/uv/python/cpython-3.11.13-macos-aarch64-none/lib/python3.11/asyncio/base_events.py", line 1936, in _run_once
    handle._run()
    │      └ <function Handle._run at 0x1034f8b80>
    └ <Handle <TaskStepMethWrapper object at 0x122664d90>()>
  File "/Users/prada/.local/share/uv/python/cpython-3.11.13-macos-aarch64-none/lib/python3.11/asyncio/events.py", line 84, in _run
    self._context.run(self._callback, *self._args)
    │    │            │    │           │    └ <member '_args' of 'Handle' objects>
    │    │            │    │           └ <Handle <TaskStepMethWrapper object at 0x122664d90>()>
    │    │            │    └ <member '_callback' of 'Handle' objects>
    │    │            └ <Handle <TaskStepMethWrapper object at 0x122664d90>()>
    │    └ <member '_context' of 'Handle' objects>
    └ <Handle <TaskStepMethWrapper object at 0x122664d90>()>
  File "/Users/prada/Desktop/coding/SIH25/.venv/lib/python3.11/site-packages/pipecat/utils/asyncio/task_manager.py", line 159, in run_coroutine
    await coroutine
          └ <coroutine object FrameProcessor.__process_frame_task_handler at 0x121538d60>
  File "/Users/prada/Desktop/coding/SIH25/.venv/lib/python3.11/site-packages/pipecat/processors/frame_processor.py", line 877, in __process_frame_task_handler
    await self.__process_frame(frame, direction, callback)
          │                    │      │          └ None
          │                    │      └ <FrameDirection.DOWNSTREAM: 1>
          │                    └ OpenAILLMContextFrame(id=1183, name='OpenAILLMContextFrame#0', pts=None, metadata={}, transport_source=None, transport_destin...
          └ <pipecat.services.groq.llm.GroqLLMService object at 0x11f067450>
> File "/Users/prada/Desktop/coding/SIH25/.venv/lib/python3.11/site-packages/pipecat/processors/frame_processor.py", line 829, in __process_frame
    await self.process_frame(frame, direction)
          │    │             │      └ <FrameDirection.DOWNSTREAM: 1>
          │    │             └ OpenAILLMContextFrame(id=1183, name='OpenAILLMContextFrame#0', pts=None, metadata={}, transport_source=None, transport_destin...
          │    └ <function BaseOpenAILLMService.process_frame at 0x11e6ffa60>
          └ <pipecat.services.groq.llm.GroqLLMService object at 0x11f067450>
  File "/Users/prada/Desktop/coding/SIH25/.venv/lib/python3.11/site-packages/pipecat/services/openai/base_llm.py", line 449, in process_frame
    await self._process_context(context)
          │    │                └ <pipecat.processors.aggregators.openai_llm_context.OpenAILLMContext object at 0x1214e3150>
          │    └ <function BaseOpenAILLMService._process_context at 0x11e6ffb00>
          └ <pipecat.services.groq.llm.GroqLLMService object at 0x11f067450>
  File "/Users/prada/Desktop/coding/SIH25/.venv/lib/python3.11/site-packages/pipecat/utils/tracing/service_decorators.py", line 498, in wrapper
    return await f(self, context, *args, **kwargs)
                 │ │     │         │       └ {}
                 │ │     │         └ ()
                 │ │     └ <pipecat.processors.aggregators.openai_llm_context.OpenAILLMContext object at 0x1214e3150>
                 │ └ <pipecat.services.groq.llm.GroqLLMService object at 0x11f067450>
                 └ <function BaseOpenAILLMService._process_context at 0x11e6ff9c0>
  File "/Users/prada/Desktop/coding/SIH25/.venv/lib/python3.11/site-packages/pipecat/services/openai/base_llm.py", line 329, in _process_context
    chunk_stream = await (
  File "/Users/prada/Desktop/coding/SIH25/.venv/lib/python3.11/site-packages/pipecat/services/openai/base_llm.py", line 299, in _stream_chat_completions_specific_context
    chunks = await self.get_chat_completions(params)
                   │    │                    └ {'messages': [{'role': 'system', 'content': 'You are FloatChat, an expert oceanographic data assistant specializing in ARGO f...
                   │    └ <function BaseOpenAILLMService.get_chat_completions at 0x11e6ff7e0>
                   └ <pipecat.services.groq.llm.GroqLLMService object at 0x11f067450>
  File "/Users/prada/Desktop/coding/SIH25/.venv/lib/python3.11/site-packages/pipecat/services/openai/base_llm.py", line 212, in get_chat_completions
    chunks = await self._client.chat.completions.create(**params)
                   │    │       │    │           │        └ {'model': 'llama-3.1-70b-versatile', 'stream': True, 'stream_options': {'include_usage': True}, 'frequency_penalty': NOT_GIVE...
                   │    │       │    │           └ <function AsyncCompletions.create at 0x1215fa160>
                   │    │       │    └ <openai.resources.chat.completions.completions.AsyncCompletions object at 0x121552fd0>
                   │    │       └ <openai.resources.chat.chat.AsyncChat object at 0x1215513d0>
                   │    └ <openai.AsyncOpenAI object at 0x11f08dd50>
                   └ <pipecat.services.groq.llm.GroqLLMService object at 0x11f067450>
  File "/Users/prada/Desktop/coding/SIH25/.venv/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py", line 2556, in create
    return await self._post(
                 │    └ <bound method AsyncAPIClient.post of <openai.AsyncOpenAI object at 0x11f08dd50>>
                 └ <openai.resources.chat.completions.completions.AsyncCompletions object at 0x121552fd0>
  File "/Users/prada/Desktop/coding/SIH25/.venv/lib/python3.11/site-packages/openai/_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
                 │    │       │        │            │                  └ openai.AsyncStream[openai.types.chat.chat_completion_chunk.ChatCompletionChunk]
                 │    │       │        │            └ True
                 │    │       │        └ FinalRequestOptions(method='post', url='/chat/completions', params={}, headers=NOT_GIVEN, max_retries=NOT_GIVEN, timeout=NOT_...
                 │    │       └ <class 'openai.types.chat.chat_completion.ChatCompletion'>
                 │    └ <function AsyncAPIClient.request at 0x10e380a40>
                 └ <openai.AsyncOpenAI object at 0x11f08dd50>
  File "/Users/prada/Desktop/coding/SIH25/.venv/lib/python3.11/site-packages/openai/_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
          │    └ <function BaseClient._make_status_error_from_response at 0x10e375bc0>
          └ <openai.AsyncOpenAI object at 0x11f08dd50>

openai.BadRequestError: Error code: 400 - {'error': {'message': 'The model `llama-3.1-70b-versatile` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
2025-09-27T17:32:52.832809+0530 | WARNING | pipecat.pipeline.task:657 | PipelineTask#0: Something went wrong: ErrorFrame#0(error: Error code: 400 - {'error': {'message': 'The model `llama-3.1-70b-versatile` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}, fatal: False)
