2025-09-27T17:33:29.870668+0530 | INFO | __main__:280 | Starting Pipecat Oceanographic Voice AI Pipeline
2025-09-27T17:33:29.870839+0530 | INFO | __main__:134 | Using Daily transport for WebRTC connection
2025-09-27T17:33:29.870930+0530 | WARNING | __main__:137 | Daily transport configuration needed - using local transport
2025-09-27T17:33:29.871000+0530 | INFO | __main__:141 | Using local microphone and speakers
2025-09-27T17:33:30.035981+0530 | WARNING | pipecat.utils.base_object:108 | Event handler bot_started_speaking not registered
2025-09-27T17:33:30.036210+0530 | WARNING | pipecat.utils.base_object:108 | Event handler bot_stopped_speaking not registered
2025-09-27T17:33:30.036312+0530 | INFO | __main__:168 | Using Soniox STT service
2025-09-27T17:33:30.036461+0530 | INFO | __main__:185 | Using Groq LLM for fast inference
2025-09-27T17:33:30.151456+0530 | INFO | __main__:211 | Using Sarvam TTS with natural voice
2025-09-27T17:33:30.178201+0530 | WARNING | __main__:242 | Could not connect to ARGO MCP server: unhandled errors in a TaskGroup (1 sub-exception)
2025-09-27T17:33:31.961561+0530 | INFO | __main__:262 | Connected to Supabase MCP server
2025-09-27T17:33:31.961734+0530 | INFO | __main__:291 | Session started: 96ae8515-d210-4967-88a0-ce11fb55f39a
2025-09-27T17:33:31.962379+0530 | INFO | __main__:332 | Pipecat oceanographic pipeline created successfully
2025-09-27T17:33:31.962781+0530 | INFO | __main__:346 | Starting voice AI conversation...
2025-09-27T17:33:42.637272+0530 | ERROR | pipecat.processors.frame_processor:834 | GroqLLMService#0: error processing frame: Error code: 400 - {'error': {'message': 'The model `llama-3.1-70b-versatile` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
Traceback (most recent call last):

  File "/Users/prada/Desktop/coding/SIH25/sih25/VOICE_AI/oceanographic_voice_pipeline.py", line 371, in <module>
    asyncio.run(main())
    │       │   └ <function main at 0x11d380ae0>
    │       └ <function run at 0x10189fd80>
    └ <module 'asyncio' from '/Users/prada/.local/share/uv/python/cpython-3.11.13-macos-aarch64-none/lib/python3.11/asyncio/__init_...

  File "/Users/prada/.local/share/uv/python/cpython-3.11.13-macos-aarch64-none/lib/python3.11/asyncio/runners.py", line 190, in run
    return runner.run(main)
           │      │   └ <coroutine object main at 0x11d295fe0>
           │      └ <function Runner.run at 0x10190bec0>
           └ <asyncio.runners.Runner object at 0x11d32fc50>
  File "/Users/prada/.local/share/uv/python/cpython-3.11.13-macos-aarch64-none/lib/python3.11/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           │    │     │                  └ <Task pending name='Task-1' coro=<main() running at /Users/prada/Desktop/coding/SIH25/sih25/VOICE_AI/oceanographic_voice_pipe...
           │    │     └ <function BaseEventLoop.run_until_complete at 0x1019099e0>
           │    └ <_UnixSelectorEventLoop running=True closed=False debug=False>
           └ <asyncio.runners.Runner object at 0x11d32fc50>
  File "/Users/prada/.local/share/uv/python/cpython-3.11.13-macos-aarch64-none/lib/python3.11/asyncio/base_events.py", line 641, in run_until_complete
    self.run_forever()
    │    └ <function BaseEventLoop.run_forever at 0x101909940>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>
  File "/Users/prada/.local/share/uv/python/cpython-3.11.13-macos-aarch64-none/lib/python3.11/asyncio/base_events.py", line 608, in run_forever
    self._run_once()
    │    └ <function BaseEventLoop._run_once at 0x10190b740>
    └ <_UnixSelectorEventLoop running=True closed=False debug=False>
  File "/Users/prada/.local/share/uv/python/cpython-3.11.13-macos-aarch64-none/lib/python3.11/asyncio/base_events.py", line 1936, in _run_once
    handle._run()
    │      └ <function Handle._run at 0x101860b80>
    └ <Handle <TaskStepMethWrapper object at 0x12093b7f0>()>
  File "/Users/prada/.local/share/uv/python/cpython-3.11.13-macos-aarch64-none/lib/python3.11/asyncio/events.py", line 84, in _run
    self._context.run(self._callback, *self._args)
    │    │            │    │           │    └ <member '_args' of 'Handle' objects>
    │    │            │    │           └ <Handle <TaskStepMethWrapper object at 0x12093b7f0>()>
    │    │            │    └ <member '_callback' of 'Handle' objects>
    │    │            └ <Handle <TaskStepMethWrapper object at 0x12093b7f0>()>
    │    └ <member '_context' of 'Handle' objects>
    └ <Handle <TaskStepMethWrapper object at 0x12093b7f0>()>
  File "/Users/prada/Desktop/coding/SIH25/.venv/lib/python3.11/site-packages/pipecat/utils/asyncio/task_manager.py", line 159, in run_coroutine
    await coroutine
          └ <coroutine object FrameProcessor.__process_frame_task_handler at 0x11f778e50>
  File "/Users/prada/Desktop/coding/SIH25/.venv/lib/python3.11/site-packages/pipecat/processors/frame_processor.py", line 877, in __process_frame_task_handler
    await self.__process_frame(frame, direction, callback)
          │                    │      │          └ None
          │                    │      └ <FrameDirection.DOWNSTREAM: 1>
          │                    └ OpenAILLMContextFrame(id=656, name='OpenAILLMContextFrame#0', pts=None, metadata={}, transport_source=None, transport_destina...
          └ <pipecat.services.groq.llm.GroqLLMService object at 0x11d3a8750>
> File "/Users/prada/Desktop/coding/SIH25/.venv/lib/python3.11/site-packages/pipecat/processors/frame_processor.py", line 829, in __process_frame
    await self.process_frame(frame, direction)
          │    │             │      └ <FrameDirection.DOWNSTREAM: 1>
          │    │             └ OpenAILLMContextFrame(id=656, name='OpenAILLMContextFrame#0', pts=None, metadata={}, transport_source=None, transport_destina...
          │    └ <function BaseOpenAILLMService.process_frame at 0x11ca87a60>
          └ <pipecat.services.groq.llm.GroqLLMService object at 0x11d3a8750>
  File "/Users/prada/Desktop/coding/SIH25/.venv/lib/python3.11/site-packages/pipecat/services/openai/base_llm.py", line 449, in process_frame
    await self._process_context(context)
          │    │                └ <pipecat.processors.aggregators.openai_llm_context.OpenAILLMContext object at 0x11d3aa3d0>
          │    └ <function BaseOpenAILLMService._process_context at 0x11ca87b00>
          └ <pipecat.services.groq.llm.GroqLLMService object at 0x11d3a8750>
  File "/Users/prada/Desktop/coding/SIH25/.venv/lib/python3.11/site-packages/pipecat/utils/tracing/service_decorators.py", line 498, in wrapper
    return await f(self, context, *args, **kwargs)
                 │ │     │         │       └ {}
                 │ │     │         └ ()
                 │ │     └ <pipecat.processors.aggregators.openai_llm_context.OpenAILLMContext object at 0x11d3aa3d0>
                 │ └ <pipecat.services.groq.llm.GroqLLMService object at 0x11d3a8750>
                 └ <function BaseOpenAILLMService._process_context at 0x11ca879c0>
  File "/Users/prada/Desktop/coding/SIH25/.venv/lib/python3.11/site-packages/pipecat/services/openai/base_llm.py", line 329, in _process_context
    chunk_stream = await (
  File "/Users/prada/Desktop/coding/SIH25/.venv/lib/python3.11/site-packages/pipecat/services/openai/base_llm.py", line 299, in _stream_chat_completions_specific_context
    chunks = await self.get_chat_completions(params)
                   │    │                    └ {'messages': [{'role': 'system', 'content': 'You are FloatChat, an expert oceanographic data assistant specializing in ARGO f...
                   │    └ <function BaseOpenAILLMService.get_chat_completions at 0x11ca877e0>
                   └ <pipecat.services.groq.llm.GroqLLMService object at 0x11d3a8750>
  File "/Users/prada/Desktop/coding/SIH25/.venv/lib/python3.11/site-packages/pipecat/services/openai/base_llm.py", line 212, in get_chat_completions
    chunks = await self._client.chat.completions.create(**params)
                   │    │       │    │           │        └ {'model': 'llama-3.1-70b-versatile', 'stream': True, 'stream_options': {'include_usage': True}, 'frequency_penalty': NOT_GIVE...
                   │    │       │    │           └ <function AsyncCompletions.create at 0x11f859f80>
                   │    │       │    └ <openai.resources.chat.completions.completions.AsyncCompletions object at 0x11f7abbd0>
                   │    │       └ <openai.resources.chat.chat.AsyncChat object at 0x11f748690>
                   │    └ <openai.AsyncOpenAI object at 0x11d3aa450>
                   └ <pipecat.services.groq.llm.GroqLLMService object at 0x11d3a8750>
  File "/Users/prada/Desktop/coding/SIH25/.venv/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py", line 2556, in create
    return await self._post(
                 │    └ <bound method AsyncAPIClient.post of <openai.AsyncOpenAI object at 0x11d3aa450>>
                 └ <openai.resources.chat.completions.completions.AsyncCompletions object at 0x11f7abbd0>
  File "/Users/prada/Desktop/coding/SIH25/.venv/lib/python3.11/site-packages/openai/_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
                 │    │       │        │            │                  └ openai.AsyncStream[openai.types.chat.chat_completion_chunk.ChatCompletionChunk]
                 │    │       │        │            └ True
                 │    │       │        └ FinalRequestOptions(method='post', url='/chat/completions', params={}, headers=NOT_GIVEN, max_retries=NOT_GIVEN, timeout=NOT_...
                 │    │       └ <class 'openai.types.chat.chat_completion.ChatCompletion'>
                 │    └ <function AsyncAPIClient.request at 0x10c690a40>
                 └ <openai.AsyncOpenAI object at 0x11d3aa450>
  File "/Users/prada/Desktop/coding/SIH25/.venv/lib/python3.11/site-packages/openai/_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
          │    └ <function BaseClient._make_status_error_from_response at 0x10c685bc0>
          └ <openai.AsyncOpenAI object at 0x11d3aa450>

openai.BadRequestError: Error code: 400 - {'error': {'message': 'The model `llama-3.1-70b-versatile` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}
2025-09-27T17:33:42.652063+0530 | WARNING | pipecat.pipeline.task:657 | PipelineTask#0: Something went wrong: ErrorFrame#0(error: Error code: 400 - {'error': {'message': 'The model `llama-3.1-70b-versatile` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}, fatal: False)
2025-09-27T17:33:53.847717+0530 | WARNING | pipecat.pipeline.runner:124 | Interruption detected. Cancelling runner PipelineRunner#0
2025-09-27T17:33:55.786627+0530 | INFO | __main__:356 | Cleaning up Pipecat voice AI session
2025-09-27T17:33:55.787533+0530 | WARNING | __main__:364 | Error closing MCP client: 'MCPClient' object has no attribute 'close'
2025-09-27T17:33:55.787795+0530 | INFO | __main__:367 | Pipecat voice AI session completed
